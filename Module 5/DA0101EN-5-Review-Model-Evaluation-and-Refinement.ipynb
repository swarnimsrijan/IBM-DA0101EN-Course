{"cells":[{"cell_type":"markdown","id":"2ca7930d-fc3a-4a9a-890e-2f09307caec3","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","\n","# Model Evaluation and Refinement\n","\n","\n","Estimated time needed: **30** minutes\n","    \n","\n","## Objectives\n","\n","After completing this lab you will be able to:\n","\n","* Evaluate and refine prediction models\n"]},{"cell_type":"markdown","id":"dbd20e16-7671-4358-8254-cb9fbb03363e","metadata":{},"source":["<h2>Table of Contents</h2>\n","<ul>\n","    <li><a href=\"#ref1\">Model Evaluation </a></li>\n","    <li><a href=\"#ref2\">Over-fitting, Under-fitting and Model Selection </a></li>\n","    <li><a href=\"#ref3\">Ridge Regression </a></li>\n","    <li><a href=\"#ref4\">Grid Search</a></li>\n","</ul>\n"]},{"cell_type":"markdown","id":"9a37d1fd-f8c0-4288-9202-56313e199665","metadata":{},"source":["This dataset was hosted on IBM Cloud object. Click <a href=\"https://cocl.us/DA101EN_object_storage\">HERE</a> for free storage.\n"]},{"cell_type":"code","execution_count":1,"id":"bf8933d2-b707-4b54-9109-00785e12d075","metadata":{},"outputs":[],"source":["#install specific version of libraries used in lab\n","#! mamba install pandas==1.3.3 -y\n","#! mamba install numpy=1.21.2 -y\n","#! mamba install sklearn=0.20.1 -y\n","#! mamba install   ipywidgets=7.4.2 -y"]},{"cell_type":"code","execution_count":2,"id":"25d5c121-b500-4b51-bf8a-7127b9c327c7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas in c:\\users\\1142s\\anaconda3\\lib\\site-packages (2.0.3)\n","Requirement already satisfied: matplotlib in c:\\users\\1142s\\anaconda3\\lib\\site-packages (3.7.2)\n","Requirement already satisfied: scipy in c:\\users\\1142s\\anaconda3\\lib\\site-packages (1.11.1)\n","Requirement already satisfied: scikit-learn in c:\\users\\1142s\\anaconda3\\lib\\site-packages (1.3.0)\n","Requirement already satisfied: seaborn in c:\\users\\1142s\\anaconda3\\lib\\site-packages (0.12.2)\n","Requirement already satisfied: ipywidgets in c:\\users\\1142s\\anaconda3\\lib\\site-packages (8.0.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n","Requirement already satisfied: numpy>=1.21.0 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: joblib>=1.1.1 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipywidgets) (6.25.0)\n","Requirement already satisfied: ipython>=6.1.0 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipywidgets) (8.15.0)\n","Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipywidgets) (5.7.1)\n","Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipywidgets) (4.0.5)\n","Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipywidgets) (3.0.5)\n","Requirement already satisfied: comm>=0.1.1 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n","Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n","Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\n","Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n","Requirement already satisfied: nest-asyncio in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n","Requirement already satisfied: psutil in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n","Requirement already satisfied: pyzmq>=20 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (23.2.0)\n","Requirement already satisfied: tornado>=6.1 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.2)\n","Requirement already satisfied: backcall in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n","Requirement already satisfied: decorator in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n","Requirement already satisfied: jedi>=0.16 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n","Requirement already satisfied: pickleshare in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n","Requirement already satisfied: pygments>=2.4.0 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n","Requirement already satisfied: stack-data in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n","Requirement already satisfied: colorama in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n","Requirement already satisfied: six>=1.5 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n","Requirement already satisfied: entrypoints in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n","Requirement already satisfied: platformdirs>=2.5 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.10.0)\n","Requirement already satisfied: pywin32>=300 in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (305.1)\n","Requirement already satisfied: wcwidth in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n","Requirement already satisfied: executing in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n","Requirement already satisfied: asttokens in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n","Requirement already satisfied: pure-eval in c:\\users\\1142s\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["#Install libraries\n","%pip install pandas matplotlib scipy scikit-learn seaborn ipywidgets"]},{"cell_type":"code","execution_count":3,"id":"def2a6fc-7390-4170-b1b4-5179f26e8b36","metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Import clean data \n","path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/module_5_auto.csv'\n","df = pd.read_csv(path)"]},{"cell_type":"code","execution_count":4,"id":"3790f399-17cf-4dd3-9c8c-d60416772abc","metadata":{},"outputs":[],"source":["df.to_csv('module_5_auto.csv')"]},{"cell_type":"markdown","id":"4cb18a95-a4e4-4917-8771-9b6fde62965b","metadata":{},"source":[" First, let's only use numeric data:\n"]},{"cell_type":"code","execution_count":5,"id":"7410b4a1-3922-4220-9685-e7c6d1553b22","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0.1</th>\n","      <th>Unnamed: 0</th>\n","      <th>symboling</th>\n","      <th>normalized-losses</th>\n","      <th>wheel-base</th>\n","      <th>length</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>curb-weight</th>\n","      <th>engine-size</th>\n","      <th>...</th>\n","      <th>stroke</th>\n","      <th>compression-ratio</th>\n","      <th>horsepower</th>\n","      <th>peak-rpm</th>\n","      <th>city-mpg</th>\n","      <th>highway-mpg</th>\n","      <th>price</th>\n","      <th>city-L/100km</th>\n","      <th>diesel</th>\n","      <th>gas</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>122</td>\n","      <td>88.6</td>\n","      <td>0.811148</td>\n","      <td>0.890278</td>\n","      <td>48.8</td>\n","      <td>2548</td>\n","      <td>130</td>\n","      <td>...</td>\n","      <td>2.68</td>\n","      <td>9.0</td>\n","      <td>111.0</td>\n","      <td>5000.0</td>\n","      <td>21</td>\n","      <td>27</td>\n","      <td>13495.0</td>\n","      <td>11.190476</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>122</td>\n","      <td>88.6</td>\n","      <td>0.811148</td>\n","      <td>0.890278</td>\n","      <td>48.8</td>\n","      <td>2548</td>\n","      <td>130</td>\n","      <td>...</td>\n","      <td>2.68</td>\n","      <td>9.0</td>\n","      <td>111.0</td>\n","      <td>5000.0</td>\n","      <td>21</td>\n","      <td>27</td>\n","      <td>16500.0</td>\n","      <td>11.190476</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>122</td>\n","      <td>94.5</td>\n","      <td>0.822681</td>\n","      <td>0.909722</td>\n","      <td>52.4</td>\n","      <td>2823</td>\n","      <td>152</td>\n","      <td>...</td>\n","      <td>3.47</td>\n","      <td>9.0</td>\n","      <td>154.0</td>\n","      <td>5000.0</td>\n","      <td>19</td>\n","      <td>26</td>\n","      <td>16500.0</td>\n","      <td>12.368421</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>164</td>\n","      <td>99.8</td>\n","      <td>0.848630</td>\n","      <td>0.919444</td>\n","      <td>54.3</td>\n","      <td>2337</td>\n","      <td>109</td>\n","      <td>...</td>\n","      <td>3.40</td>\n","      <td>10.0</td>\n","      <td>102.0</td>\n","      <td>5500.0</td>\n","      <td>24</td>\n","      <td>30</td>\n","      <td>13950.0</td>\n","      <td>9.791667</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>164</td>\n","      <td>99.4</td>\n","      <td>0.848630</td>\n","      <td>0.922222</td>\n","      <td>54.3</td>\n","      <td>2824</td>\n","      <td>136</td>\n","      <td>...</td>\n","      <td>3.40</td>\n","      <td>8.0</td>\n","      <td>115.0</td>\n","      <td>5500.0</td>\n","      <td>18</td>\n","      <td>22</td>\n","      <td>17450.0</td>\n","      <td>13.055556</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 21 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0.1  Unnamed: 0  symboling  normalized-losses  wheel-base  \\\n","0             0           0          3                122        88.6   \n","1             1           1          3                122        88.6   \n","2             2           2          1                122        94.5   \n","3             3           3          2                164        99.8   \n","4             4           4          2                164        99.4   \n","\n","     length     width  height  curb-weight  engine-size  ...  stroke  \\\n","0  0.811148  0.890278    48.8         2548          130  ...    2.68   \n","1  0.811148  0.890278    48.8         2548          130  ...    2.68   \n","2  0.822681  0.909722    52.4         2823          152  ...    3.47   \n","3  0.848630  0.919444    54.3         2337          109  ...    3.40   \n","4  0.848630  0.922222    54.3         2824          136  ...    3.40   \n","\n","   compression-ratio  horsepower  peak-rpm  city-mpg  highway-mpg    price  \\\n","0                9.0       111.0    5000.0        21           27  13495.0   \n","1                9.0       111.0    5000.0        21           27  16500.0   \n","2                9.0       154.0    5000.0        19           26  16500.0   \n","3               10.0       102.0    5500.0        24           30  13950.0   \n","4                8.0       115.0    5500.0        18           22  17450.0   \n","\n","   city-L/100km  diesel  gas  \n","0     11.190476       0    1  \n","1     11.190476       0    1  \n","2     12.368421       0    1  \n","3      9.791667       0    1  \n","4     13.055556       0    1  \n","\n","[5 rows x 21 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df=df._get_numeric_data()\n","df.head()"]},{"cell_type":"markdown","id":"3f4e1213-9c4b-4e95-852d-0ea01740d1cb","metadata":{},"source":[" Libraries for plotting:\n"]},{"cell_type":"code","execution_count":6,"id":"9de9e680-708c-412c-a24d-075850364793","metadata":{},"outputs":[],"source":["from ipywidgets import interact, interactive, fixed, interact_manual"]},{"cell_type":"markdown","id":"9ee31ff5-7394-48b8-a17e-616858b5d1b2","metadata":{},"source":["<h2>Functions for Plotting</h2>\n"]},{"cell_type":"code","execution_count":7,"id":"65d2b3a7-f48f-4a1e-8206-e56be9c1f9eb","metadata":{},"outputs":[],"source":["def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n","    width = 12\n","    height = 10\n","    plt.figure(figsize=(width, height))\n","\n","    ax1 = sns.kdeplot(RedFunction, color=\"r\", label=RedName)\n","    ax2 = sns.kdeplot(BlueFunction, color=\"b\", label=BlueName, ax=ax1)\n","\n","    plt.title(Title)\n","    plt.xlabel('Price (in dollars)')\n","    plt.ylabel('Proportion of Cars')\n","    plt.show()\n","    plt.close()"]},{"cell_type":"code","execution_count":8,"id":"db0f2146-f18b-48bf-8f55-ba930ccb92df","metadata":{},"outputs":[],"source":["def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):\n","    width = 12\n","    height = 10\n","    plt.figure(figsize=(width, height))\n","    \n","    \n","    #training data \n","    #testing data \n","    # lr:  linear regression object \n","    #poly_transform:  polynomial transformation object \n"," \n","    xmax=max([xtrain.values.max(), xtest.values.max()])\n","\n","    xmin=min([xtrain.values.min(), xtest.values.min()])\n","\n","    x=np.arange(xmin, xmax, 0.1)\n","\n","\n","    plt.plot(xtrain, y_train, 'ro', label='Training Data')\n","    plt.plot(xtest, y_test, 'go', label='Test Data')\n","    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n","    plt.ylim([-10000, 60000])\n","    plt.ylabel('Price')\n","    plt.legend()"]},{"cell_type":"markdown","id":"04113961-fb60-4845-a357-6ddb01621c31","metadata":{},"source":["<h2 id=\"ref1\">Part 1: Training and Testing</h2>\n","\n","<p>An important step in testing your model is to split your data into training and testing data. We will place the target data <b>price</b> in a separate dataframe <b>y_data</b>:</p>\n"]},{"cell_type":"code","execution_count":9,"id":"1e6a2110-0e56-4570-8153-be5f7bda4f36","metadata":{},"outputs":[],"source":["y_data = df['price']"]},{"cell_type":"markdown","id":"32a00a22-719c-48ba-aa46-049c08dc69bd","metadata":{},"source":["Drop price data in dataframe **x_data**:\n"]},{"cell_type":"code","execution_count":10,"id":"cd8db659-04d2-46f9-bccb-916b032764af","metadata":{},"outputs":[],"source":["x_data=df.drop('price',axis=1)"]},{"cell_type":"markdown","id":"8b8d3782-1c2c-4b13-955c-af0d77856032","metadata":{},"source":["Now, we randomly split our data into training and testing data using the function <b>train_test_split</b>. \n"]},{"cell_type":"code","execution_count":11,"id":"e7023fda-49b6-4261-828e-9df8888cf746","metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","\n","x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.10, random_state=1)\n","\n","\n","print(\"number of test samples :\", x_test.shape[0])\n","print(\"number of training samples:\",x_train.shape[0])\n"]},{"cell_type":"markdown","id":"979eab00-0cca-4716-ae6a-ec1ee2f1c4e4","metadata":{},"source":["The <b>test_size</b> parameter sets the proportion of data that is split into the testing set. In the above, the testing set is 10% of the total dataset. \n"]},{"cell_type":"markdown","id":"0be240d1-504f-492b-bce0-6f174d68b974","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #1):</h1>\n","\n","<b>Use the function \"train_test_split\" to split up the dataset such that 40% of the data samples will be utilized for testing. Set the parameter \"random_state\" equal to zero. The output of the function should be the following:  \"x_train1\" , \"x_test1\", \"y_train1\" and  \"y_test1\".</b>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"id":"27e00690-7082-47ee-8c28-1f33e02873ed","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"edb59fce-b8a9-4a43-b11d-efab9a1eaf09","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","x_train1, x_test1, y_train1, y_test1 = train_test_split(x_data, y_data, test_size=0.4, random_state=0) \n","print(\"number of test samples :\", x_test1.shape[0])\n","print(\"number of training samples:\",x_train1.shape[0])\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"410f04da-a1cf-4ece-9f5e-7b7effb635e9","metadata":{},"source":["Let's import <b>LinearRegression</b> from the module <b>linear_model</b>.\n"]},{"cell_type":"code","execution_count":null,"id":"cddf2e5e-bde0-44f9-b675-8f7dc5139938","metadata":{},"outputs":[],"source":["from sklearn.linear_model import LinearRegression"]},{"cell_type":"markdown","id":"400a86a9-4b76-4b3f-9a52-a7795e42ceea","metadata":{},"source":[" We create a Linear Regression object:\n"]},{"cell_type":"code","execution_count":null,"id":"b78e6749-d432-4349-b56f-5c397d279435","metadata":{},"outputs":[],"source":["lre=LinearRegression()"]},{"cell_type":"markdown","id":"b197751b-8c37-42de-afb5-bec88499383d","metadata":{},"source":["We fit the model using the feature \"horsepower\":\n"]},{"cell_type":"code","execution_count":null,"id":"40ddc9e2-258b-4c33-8fa3-fef7d396e4a8","metadata":{},"outputs":[],"source":["lre.fit(x_train[['horsepower']], y_train)"]},{"cell_type":"markdown","id":"b0fe1a40-3804-4e5d-8001-e459fc5b0aec","metadata":{},"source":["Let's calculate the R^2 on the test data:\n"]},{"cell_type":"code","execution_count":null,"id":"4de3aa48-a5bd-4d9b-b815-24b16ea08603","metadata":{},"outputs":[],"source":["lre.score(x_test[['horsepower']], y_test)"]},{"cell_type":"markdown","id":"af8271b1-1dab-492b-86b6-6dca43d1ff92","metadata":{},"source":["We can see the R^2 is much smaller using the test data compared to the training data.\n"]},{"cell_type":"code","execution_count":null,"id":"26e73091-c3f5-42e1-ab29-4da381b3f369","metadata":{},"outputs":[],"source":["lre.score(x_train[['horsepower']], y_train)"]},{"cell_type":"markdown","id":"941afefc-54cb-4f3d-b9f1-687289739702","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #2): </h1>\n","<b> \n","Find the R^2  on the test data using 40% of the dataset for testing.\n","</b>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"id":"5db8d7b1-0711-4a01-936e-e30f4d02b486","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"63a62566-214d-49e4-9aa3-1f8944ad19a1","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","x_train1, x_test1, y_train1, y_test1 = train_test_split(x_data, y_data, test_size=0.4, random_state=0)\n","lre.fit(x_train1[['horsepower']],y_train1)\n","lre.score(x_test1[['horsepower']],y_test1)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"cfa8847b-7e24-40ef-bde8-9c466e76ad3f","metadata":{},"source":["Sometimes you do not have sufficient testing data; as a result, you may want to perform cross-validation. Let's go over several methods that you can use for cross-validation. \n"]},{"cell_type":"markdown","id":"cd95b504-2b16-4ce8-973e-b8ba167f20e7","metadata":{},"source":["<h2>Cross-Validation Score</h2>\n"]},{"cell_type":"markdown","id":"9be7a15d-c5f1-4ba2-a6ed-f78d435ac2b2","metadata":{},"source":["Let's import <b>cross_val_score</b> from the module <b>model_selection</b>.\n"]},{"cell_type":"code","execution_count":null,"id":"249b907d-55f6-4fb8-b632-be2d18d7fd5f","metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_val_score"]},{"cell_type":"markdown","id":"de2165e6-5691-4d07-94a5-520d26ddcdb8","metadata":{},"source":["We input the object, the feature (\"horsepower\"), and the target data (y_data). The parameter 'cv' determines the number of folds. In this case, it is 4. \n"]},{"cell_type":"code","execution_count":null,"id":"34666f89-a00b-42c9-b72e-95d750501922","metadata":{},"outputs":[],"source":["Rcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv=4)"]},{"cell_type":"markdown","id":"f40b8344-f6ee-4536-abe6-a72e414ce20c","metadata":{},"source":["The default scoring is R^2. Each element in the array has the average R^2 value for the fold:\n"]},{"cell_type":"code","execution_count":null,"id":"a5ff15c2-d996-4beb-85e7-7958aa7987d6","metadata":{},"outputs":[],"source":["Rcross"]},{"cell_type":"markdown","id":"14b609b6-45a3-4510-97e1-60c5f26f6edc","metadata":{},"source":[" We can calculate the average and standard deviation of our estimate:\n"]},{"cell_type":"code","execution_count":null,"id":"18f3f8b6-818b-4e4a-bd74-6c17e6966b1d","metadata":{},"outputs":[],"source":["print(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())"]},{"cell_type":"markdown","id":"427a8577-3786-4ccf-9c75-f541abd0da4d","metadata":{},"source":["We can use negative squared error as a score by setting the parameter  'scoring' metric to 'neg_mean_squared_error'. \n"]},{"cell_type":"code","execution_count":null,"id":"57c3c437-8479-4eef-8ab7-8a83aa1d0172","metadata":{},"outputs":[],"source":["-1 * cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')"]},{"cell_type":"markdown","id":"81ca4276-5ed5-4675-8859-46e4733e53bb","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #3): </h1>\n","<b> \n","Calculate the average R^2 using two folds, then find the average R^2 for the second fold utilizing the \"horsepower\" feature: \n","</b>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"id":"9a0aa3c3-7407-4e21-90db-3f8a7bfacb7d","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"e1c2997c-36d9-4960-a106-ce3082e0d6ec","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","Rc=cross_val_score(lre,x_data[['horsepower']], y_data,cv=2)\n","Rc.mean()\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"16a9b970-3cfc-4b6f-a1f6-0113ca6ecdd0","metadata":{},"source":["You can also use the function 'cross_val_predict' to predict the output. The function splits up the data into the specified number of folds, with one fold for testing and the other folds are used for training. First, import the function:\n"]},{"cell_type":"code","execution_count":null,"id":"c056e175-2f5c-4f23-8c01-620c3a142f5b","metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_val_predict"]},{"cell_type":"markdown","id":"bfb73fff-5f45-4cac-8a7e-72d454a27143","metadata":{},"source":["We input the object, the feature <b>\"horsepower\"</b>, and the target data <b>y_data</b>. The parameter 'cv' determines the number of folds. In this case, it is 4. We can produce an output:\n"]},{"cell_type":"code","execution_count":null,"id":"a5ed4726-401b-4918-9581-f26fead16fe9","metadata":{},"outputs":[],"source":["yhat = cross_val_predict(lre,x_data[['horsepower']], y_data,cv=4)\n","yhat[0:5]"]},{"cell_type":"markdown","id":"ab83ac6d-0f84-48b4-98dd-25cd535c660e","metadata":{},"source":["<h2 id=\"ref2\">Part 2: Overfitting, Underfitting and Model Selection</h2>\n","\n","<p>It turns out that the test data, sometimes referred to as the \"out of sample data\", is a much better measure of how well your model performs in the real world.  One reason for this is overfitting.\n","\n","Let's go over some examples. It turns out these differences are more apparent in Multiple Linear Regression and Polynomial Regression so we will explore overfitting in that context.</p>\n"]},{"cell_type":"markdown","id":"0b961fe4-d48e-4d52-8c29-5ca7f06553eb","metadata":{},"source":["Let's create Multiple Linear Regression objects and train the model using <b>'horsepower'</b>, <b>'curb-weight'</b>, <b>'engine-size'</b> and <b>'highway-mpg'</b> as features.\n"]},{"cell_type":"code","execution_count":null,"id":"e16d857c-858f-489f-b39e-3103c390da56","metadata":{},"outputs":[],"source":["lr = LinearRegression()\n","lr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)"]},{"cell_type":"markdown","id":"d0d1e9dd-e3d5-45bd-baed-4269c7a25efc","metadata":{},"source":["Prediction using training data:\n"]},{"cell_type":"code","execution_count":null,"id":"ccd494b9-077f-4470-9a79-53bd6e6bee4f","metadata":{},"outputs":[],"source":["yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n","yhat_train[0:5]"]},{"cell_type":"markdown","id":"1626f70c-02de-4af0-bbdc-07a5db637847","metadata":{},"source":["Prediction using test data: \n"]},{"cell_type":"code","execution_count":null,"id":"05f3af89-7922-4c19-b636-2ece1ba0676a","metadata":{},"outputs":[],"source":["yhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n","yhat_test[0:5]"]},{"cell_type":"markdown","id":"52613796-251e-48c8-b959-8c9ebcc0b722","metadata":{},"source":["Let's perform some model evaluation using our training and testing data separately. First, we import the seaborn and matplotlib library for plotting.\n"]},{"cell_type":"code","execution_count":null,"id":"ad339f08-650e-4390-bdbf-17c39043b6cc","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns"]},{"cell_type":"markdown","id":"860da42c-8084-4864-943e-4b24c224149a","metadata":{},"source":["Let's examine the distribution of the predicted values of the training data.\n"]},{"cell_type":"code","execution_count":null,"id":"daeb28cb-e2be-4b1b-8dfa-e7de02afcd57","metadata":{},"outputs":[],"source":["Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'\n","DistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)"]},{"cell_type":"markdown","id":"a6ab68e8-8434-4aae-b326-37b00f6a5d84","metadata":{},"source":["Figure 1: Plot of predicted values using the training data compared to the actual values of the training data. \n"]},{"cell_type":"markdown","id":"9cc9fdf4-2b79-428d-af92-d0f8269b58f1","metadata":{},"source":["So far, the model seems to be doing well in learning from the training dataset. But what happens when the model encounters new data from the testing dataset? When the model generates new values from the test data, we see the distribution of the predicted values is much different from the actual target values. \n"]},{"cell_type":"code","execution_count":null,"id":"3432d82a-5108-4c63-8230-b6ac939bdf4e","metadata":{},"outputs":[],"source":["Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\n","DistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)"]},{"cell_type":"markdown","id":"83df4c3a-5967-4a05-9fb6-a280d092cfea","metadata":{},"source":["Figure 2: Plot of predicted value using the test data compared to the actual values of the test data. \n"]},{"cell_type":"markdown","id":"dedbeede-6a3e-4e6c-b26c-0748f277d43b","metadata":{},"source":["<p>Comparing Figure 1 and Figure 2, it is evident that the distribution of the test data in Figure 1 is much better at fitting the data. This difference in Figure 2 is apparent in the range of 5000 to 15,000. This is where the shape of the distribution is extremely different. Let's see if polynomial regression also exhibits a drop in the prediction accuracy when analysing the test dataset.</p>\n"]},{"cell_type":"code","execution_count":null,"id":"40c0412a-2f26-466f-9a6a-298f3aec8c08","metadata":{},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures"]},{"cell_type":"markdown","id":"525786fe-53cc-494e-b6fc-2d1fea55c9ee","metadata":{},"source":["<h4>Overfitting</h4>\n","<p>Overfitting occurs when the model fits the noise, but not the underlying process. Therefore, when testing your model using the test set, your model does not perform as well since it is modelling noise, not the underlying process that generated the relationship. Let's create a degree 5 polynomial model.</p>\n"]},{"cell_type":"markdown","id":"c2a5dc50-ecb8-44c5-86fb-e266a691cb94","metadata":{},"source":["Let's use 55 percent of the data for training and the rest for testing:\n"]},{"cell_type":"code","execution_count":null,"id":"bcc607c6-dbbe-4bb3-8932-59c1cbf6a213","metadata":{},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)"]},{"cell_type":"markdown","id":"fbdc4e12-57cb-4b1c-9ecf-6034a8e592f1","metadata":{},"source":["We will perform a degree 5 polynomial transformation on the feature <b>'horsepower'</b>. \n"]},{"cell_type":"code","execution_count":null,"id":"aec078e8-4ef6-4d11-aac9-a671f142652f","metadata":{},"outputs":[],"source":["pr = PolynomialFeatures(degree=5)\n","x_train_pr = pr.fit_transform(x_train[['horsepower']])\n","x_test_pr = pr.fit_transform(x_test[['horsepower']])\n","pr"]},{"cell_type":"markdown","id":"e35b7e7b-f626-4dc6-8916-3964d24df187","metadata":{},"source":["Now, let's create a Linear Regression model \"poly\" and train it.\n"]},{"cell_type":"code","execution_count":null,"id":"8e9bdcda-ddfd-4eee-8b53-506727114ccb","metadata":{},"outputs":[],"source":["poly = LinearRegression()\n","poly.fit(x_train_pr, y_train)"]},{"cell_type":"markdown","id":"4832df33-2b05-4082-baf1-14abf08d6a09","metadata":{},"source":["We can see the output of our model using the method \"predict.\" We assign the values to \"yhat\".\n"]},{"cell_type":"code","execution_count":null,"id":"4542a459-4f83-40a1-9858-0ec77876b67f","metadata":{},"outputs":[],"source":["yhat = poly.predict(x_test_pr)\n","yhat[0:5]"]},{"cell_type":"markdown","id":"780e261b-0774-4208-ab3a-27e95849d1f9","metadata":{},"source":["Let's take the first five predicted values and compare it to the actual targets. \n"]},{"cell_type":"code","execution_count":null,"id":"812468c9-d62a-491b-844d-fc4ba1a46c6f","metadata":{},"outputs":[],"source":["print(\"Predicted values:\", yhat[0:4])\n","print(\"True values:\", y_test[0:4].values)"]},{"cell_type":"markdown","id":"651c054c-7211-42b5-954c-2d9c074e9726","metadata":{},"source":["We will use the function \"PollyPlot\" that we defined at the beginning of the lab to display the training data, testing data, and the predicted function.\n"]},{"cell_type":"code","execution_count":null,"id":"a63bbe57-6728-4a70-a7ed-71b27429c621","metadata":{},"outputs":[],"source":["PollyPlot(x_train['horsepower'], x_test['horsepower'], y_train, y_test, poly,pr)"]},{"cell_type":"markdown","id":"d4bece3b-fe21-4a66-811f-17317bb56335","metadata":{},"source":["Figure 3: A polynomial regression model where red dots represent training data, green dots represent test data, and the blue line represents the model prediction. \n"]},{"cell_type":"markdown","id":"ab0c99ea-e751-445c-91fa-251f2f611f47","metadata":{},"source":["We see that the estimated function appears to track the data but around 200 horsepower, the function begins to diverge from the data points. \n"]},{"cell_type":"markdown","id":"e07d6751-f518-4951-b3e6-09b2d00f4e0a","metadata":{},"source":[" R^2 of the training data:\n"]},{"cell_type":"code","execution_count":null,"id":"b7a38bf6-664e-4ad0-89ed-47d4eed3a0d7","metadata":{},"outputs":[],"source":["poly.score(x_train_pr, y_train)"]},{"cell_type":"markdown","id":"716159d8-99b9-4c92-8434-5b5fad11354c","metadata":{},"source":[" R^2 of the test data:\n"]},{"cell_type":"code","execution_count":null,"id":"c87a6612-6139-43e7-af1b-21d83a6e635b","metadata":{},"outputs":[],"source":["poly.score(x_test_pr, y_test)"]},{"cell_type":"markdown","id":"227d023b-eb84-4f3d-a085-32117b6ae2d9","metadata":{},"source":["We see the R^2 for the training data is 0.5567 while the R^2 on the test data was -29.87.  The lower the R^2, the worse the model. A negative R^2 is a sign of overfitting.\n"]},{"cell_type":"markdown","id":"0498b7ad-8bc1-4232-be5c-85c8aec3537d","metadata":{},"source":["Let's see how the R^2 changes on the test data for different order polynomials and then plot the results:\n"]},{"cell_type":"code","execution_count":null,"id":"bd3b278a-99d6-4461-9a8b-d67b7b096607","metadata":{},"outputs":[],"source":["Rsqu_test = []\n","\n","order = [1, 2, 3, 4]\n","for n in order:\n","    pr = PolynomialFeatures(degree=n)\n","    \n","    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n","    \n","    x_test_pr = pr.fit_transform(x_test[['horsepower']])    \n","    \n","    lr.fit(x_train_pr, y_train)\n","    \n","    Rsqu_test.append(lr.score(x_test_pr, y_test))\n","\n","plt.plot(order, Rsqu_test)\n","plt.xlabel('order')\n","plt.ylabel('R^2')\n","plt.title('R^2 Using Test Data')\n","plt.text(3, 0.75, 'Maximum R^2 ')    "]},{"cell_type":"markdown","id":"beece08d-d7a6-4c55-964e-28dba9c864e1","metadata":{},"source":["We see the R^2 gradually increases until an order three polynomial is used. Then, the R^2 dramatically decreases at an order four polynomial.\n"]},{"cell_type":"markdown","id":"8c66b1c7-1804-46d6-9229-38cf0c5784f2","metadata":{},"source":["The following function will be used in the next section. Please run the cell below.\n"]},{"cell_type":"code","execution_count":null,"id":"7a4a7504-0f6d-419d-a96b-503b4c66a3b4","metadata":{},"outputs":[],"source":["def f(order, test_data):\n","    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n","    pr = PolynomialFeatures(degree=order)\n","    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n","    x_test_pr = pr.fit_transform(x_test[['horsepower']])\n","    poly = LinearRegression()\n","    poly.fit(x_train_pr,y_train)\n","    PollyPlot(x_train['horsepower'], x_test['horsepower'], y_train,y_test, poly, pr)"]},{"cell_type":"markdown","id":"096b280f-f017-4b76-98c7-67d5d4491c2c","metadata":{},"source":["The following interface allows you to experiment with different polynomial orders and different amounts of data. \n"]},{"cell_type":"code","execution_count":null,"id":"950e3a5d-5228-4f6f-b0b3-79377304dc61","metadata":{},"outputs":[],"source":["interact(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))"]},{"cell_type":"markdown","id":"8bb64232-bc71-4154-9d87-89b4eeba17af","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #4a):</h1>\n","\n","<b>We can perform polynomial transformations with more than one feature. Create a \"PolynomialFeatures\" object \"pr1\" of degree two.</b>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"id":"f8dedbb4-6922-42ac-9ff2-30b30c755044","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"1bcfdfde-a6eb-45c1-9d84-e7bcf928becd","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","pr1=PolynomialFeatures(degree=2)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"63fe0de3-9ff7-4f7e-be04-3858ba4a09ca","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #4b): </h1>\n","\n","<b> \n"," Transform the training and testing samples for the features 'horsepower', 'curb-weight', 'engine-size' and 'highway-mpg'. Hint: use the method \"fit_transform\".</b>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"id":"71f49f07-b58b-4de4-b278-896ee7bb97c1","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"b0bb1c5c-2632-4df5-b7a5-6f77721fd2a5","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","x_train_pr1=pr1.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n","\n","x_test_pr1=pr1.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n","\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"be4f6d81-554e-42a3-aa2f-2558fef10159","metadata":{},"source":["<!-- The answer is below:\n","\n","x_train_pr1=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n","x_test_pr1=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n","\n","-->\n"]},{"cell_type":"markdown","id":"e7f6d393-befc-48e8-bfe8-b7bb5c446927","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #4c): </h1>\n","<b> \n","How many dimensions does the new feature have? Hint: use the attribute \"shape\".\n","</b>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"id":"f84768b8-205e-4461-89cc-4b3e74fc6960","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"bf3dc257-8aab-4530-a071-4101e26247e3","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","x_train_pr1.shape #there are now 15 features\n","\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"66399294-c696-45a0-968b-52388188a8a2","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #4d): </h1>\n","\n","<b> \n","Create a linear regression model \"poly1\". Train the object using the method \"fit\" using the polynomial features.</b>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"id":"bd70bf29-eb4d-40f3-a9ef-ed8f5f0550cf","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"29371a63-ff1f-4b2a-a27b-d1d97f1718bd","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","poly1=LinearRegression().fit(x_train_pr1,y_train)\n","\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"7edf29f7-e539-4c62-b610-8f1343ec7801","metadata":{},"source":[" <div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #4e): </h1>\n","<b>Use the method  \"predict\" to predict an output on the polynomial features, then use the function \"DistributionPlot\" to display the distribution of the predicted test output vs. the actual test data.</b>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"id":"940ae243-1faf-4a4b-8349-6e65c8279145","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"577e0b9b-9b41-4973-ab83-e74a7637b477","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","yhat_test1=poly1.predict(x_test_pr1)\n","\n","Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\n","\n","DistributionPlot(y_test, yhat_test1, \"Actual Values (Test)\", \"Predicted Values (Test)\", Title)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"ce45ac75-ed06-4692-9f15-a6d3b536a49d","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #4f): </h1>\n","\n","<b>Using the distribution plot above, describe (in words) the two regions where the predicted prices are less accurate than the actual prices.</b>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"id":"83df56ce-0839-4d2d-86b1-3c650b7b3671","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"04950975-5a93-4070-8d75-cdb177cadaf5","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","#The predicted value is higher than actual value for cars where the price $10,000 range, conversely the predicted price is lower than the price cost in the $30,000 to $40,000 range. As such the model is not as accurate in these ranges.\n","\n","```\n","\n","</details>\n","\n"]},{"cell_type":"markdown","id":"bc4a26dc-6d9c-4f5f-9480-77be90f66daf","metadata":{},"source":["<h2 id=\"ref3\">Part 3: Ridge Regression</h2> \n"]},{"cell_type":"markdown","id":"1e55c98d-eee8-4b8f-86fa-285f2b2c04df","metadata":{},"source":[" In this section, we will review Ridge Regression and see how the parameter alpha changes the model. Just a note, here our test data will be used as validation data.\n"]},{"cell_type":"markdown","id":"0cf1711a-b9e3-43bf-b369-528481243eb7","metadata":{},"source":[" Let's perform a degree two polynomial transformation on our data. \n"]},{"cell_type":"code","execution_count":null,"id":"7439874f-e482-431e-a56d-554fe6cc01f3","metadata":{},"outputs":[],"source":["pr=PolynomialFeatures(degree=2)\n","x_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\n","x_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])"]},{"cell_type":"markdown","id":"ce55e34b-b253-4f48-baca-4d23df44ab2f","metadata":{},"source":[" Let's import  <b>Ridge</b>  from the module <b>linear models</b>.\n"]},{"cell_type":"code","execution_count":null,"id":"37733129-522e-4fba-ac2d-83d00dae7605","metadata":{},"outputs":[],"source":["from sklearn.linear_model import Ridge"]},{"cell_type":"markdown","id":"29fccf37-8eda-4ecf-943f-557cde26ba51","metadata":{},"source":["Let's create a Ridge regression object, setting the regularization parameter (alpha) to 1 \n"]},{"cell_type":"code","execution_count":null,"id":"d62f555f-d098-49a7-9b36-6f95ba2f2c16","metadata":{},"outputs":[],"source":["RigeModel=Ridge(alpha=1)"]},{"cell_type":"markdown","id":"76d463bb-0540-4018-8683-e3390e2865b7","metadata":{},"source":["Like regular regression, you can fit the model using the method <b>fit</b>.\n"]},{"cell_type":"code","execution_count":null,"id":"bf832bd5-ab56-4ac4-a39c-fa1475523c16","metadata":{},"outputs":[],"source":["RigeModel.fit(x_train_pr, y_train)"]},{"cell_type":"markdown","id":"6d25b85d-03bf-4eaa-8f36-8df542b0fdd4","metadata":{},"source":[" Similarly, you can obtain a prediction: \n"]},{"cell_type":"code","execution_count":null,"id":"04a26e79-9662-4b6f-a0db-e3c924f094c1","metadata":{},"outputs":[],"source":["yhat = RigeModel.predict(x_test_pr)"]},{"cell_type":"markdown","id":"6853122f-3315-4dfc-8f32-72cc92e5658c","metadata":{},"source":["Let's compare the first four predicted samples to our test set: \n"]},{"cell_type":"code","execution_count":null,"id":"a0b1b010-0863-4558-8382-bb3d220313c2","metadata":{},"outputs":[],"source":["print('predicted:', yhat[0:4])\n","print('test set :', y_test[0:4].values)"]},{"cell_type":"markdown","id":"d57a7b97-d4b0-46cb-821b-794c957baf18","metadata":{},"source":["We select the value of alpha that minimizes the test error. To do so, we can use a for loop. We have also created a progress bar to see how many iterations we have completed so far.\n"]},{"cell_type":"code","execution_count":null,"id":"da7dfa39-87b9-41f8-a664-066db697d662","metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","\n","Rsqu_test = []\n","Rsqu_train = []\n","dummy1 = []\n","Alpha = 10 * np.array(range(0,1000))\n","pbar = tqdm(Alpha)\n","\n","for alpha in pbar:\n","    RigeModel = Ridge(alpha=alpha) \n","    RigeModel.fit(x_train_pr, y_train)\n","    test_score, train_score = RigeModel.score(x_test_pr, y_test), RigeModel.score(x_train_pr, y_train)\n","    \n","    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n","\n","    Rsqu_test.append(test_score)\n","    Rsqu_train.append(train_score)"]},{"cell_type":"markdown","id":"b737b966-e62e-406a-85b2-537b2ab4f709","metadata":{},"source":["We can plot out the value of R^2 for different alphas: \n"]},{"cell_type":"code","execution_count":null,"id":"81660dd4-7819-47c5-bc31-6870e5039db6","metadata":{},"outputs":[],"source":["width = 12\n","height = 10\n","plt.figure(figsize=(width, height))\n","\n","plt.plot(Alpha,Rsqu_test, label='validation data  ')\n","plt.plot(Alpha,Rsqu_train, 'r', label='training Data ')\n","plt.xlabel('alpha')\n","plt.ylabel('R^2')\n","plt.legend()"]},{"cell_type":"markdown","id":"a4f9268a-95f3-4ccb-93f9-aec4eb3f41c3","metadata":{},"source":["**Figure 4**: The blue line represents the R^2 of the validation data, and the red line represents the R^2 of the training data. The x-axis represents the different values of Alpha. \n"]},{"cell_type":"markdown","id":"1ce6f7d3-1942-4e56-9751-1b092745f70c","metadata":{},"source":["Here the model is built and tested on the same data, so the training and test data are the same.\n","\n","The red line in Figure 4 represents the R^2 of the training data. As alpha increases the R^2 decreases. Therefore, as alpha increases, the model performs worse on the training data\n","\n","The blue line represents the R^2 on the validation data. As the value for alpha increases, the R^2 increases and converges at a point.\n"]},{"cell_type":"markdown","id":"1504287d-7547-4660-9dac-bcae603e93b9","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #5): </h1>\n","\n","Perform Ridge regression. Calculate the R^2 using the polynomial features, use the training data to train the model and use the test data to test the model. The parameter alpha should be set to 10.\n","</div>\n"]},{"cell_type":"code","execution_count":null,"id":"01f02eb7-da15-43c0-aa73-1a21e9fd75bd","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"9d0ce1c7-a89a-45b9-a022-fe47879cade2","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","RigeModel = Ridge(alpha=10) \n","RigeModel.fit(x_train_pr, y_train)\n","RigeModel.score(x_test_pr, y_test)\n","\n","```\n","\n","</details>\n","\n"]},{"cell_type":"markdown","id":"ed247f53-9808-42a0-bd96-153aee25d8ec","metadata":{},"source":["<h2 id=\"ref4\">Part 4: Grid Search</h2>\n"]},{"cell_type":"markdown","id":"6df3640c-63cc-4510-92a7-1407ad55a54d","metadata":{},"source":["The term alpha is a hyperparameter. Sklearn has the class <b>GridSearchCV</b> to make the process of finding the best hyperparameter simpler.\n"]},{"cell_type":"markdown","id":"128556b6-7746-4ab9-ab3b-a171d6dbaa61","metadata":{},"source":["Let's import <b>GridSearchCV</b> from  the module <b>model_selection</b>.\n"]},{"cell_type":"code","execution_count":null,"id":"ca26ce33-fd4c-4913-904e-8c873832d214","metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV"]},{"cell_type":"markdown","id":"44a87768-c686-4ab2-b3da-ef2271e9048c","metadata":{},"source":["We create a dictionary of parameter values:\n"]},{"cell_type":"code","execution_count":null,"id":"387fdc56-ab72-45d4-a0dd-c230e3b5c36a","metadata":{},"outputs":[],"source":["parameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]\n","parameters1"]},{"cell_type":"markdown","id":"9ac4673c-9750-4298-a56d-dee50d7a527a","metadata":{},"source":["Create a Ridge regression object:\n"]},{"cell_type":"code","execution_count":null,"id":"48a1bd59-6383-46d4-9810-de0a58e10dd8","metadata":{},"outputs":[],"source":["RR=Ridge()\n","RR"]},{"cell_type":"markdown","id":"fd2f8b33-40f8-490e-89b0-4820a0461b85","metadata":{},"source":["Create a ridge grid search object:\n"]},{"cell_type":"code","execution_count":null,"id":"4b4d2929-50b5-4f19-8d60-485ea9e11c40","metadata":{},"outputs":[],"source":["Grid1 = GridSearchCV(RR, parameters1,cv=4)"]},{"cell_type":"markdown","id":"bae0031a-fb0c-45c4-aeab-187c7774286d","metadata":{},"source":["In order to avoid a deprecation warning due to the iid parameter, we set the value of iid to \"None\".\n","\n","Fit the model:\n"]},{"cell_type":"code","execution_count":null,"id":"f1504793-c69d-40c2-8bf8-fec22c6e08e7","metadata":{},"outputs":[],"source":["Grid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)"]},{"cell_type":"markdown","id":"5a16d478-ad07-4cdf-8c11-1fd680ec2abb","metadata":{},"source":["The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:\n"]},{"cell_type":"code","execution_count":null,"id":"4bd6c0f8-2833-4dc1-9896-bec8f4a93d5d","metadata":{},"outputs":[],"source":["BestRR=Grid1.best_estimator_\n","BestRR"]},{"cell_type":"markdown","id":"457c14a0-47c5-4c99-b0e6-e663d77cf1e8","metadata":{},"source":[" We now test our model on the test data:\n"]},{"cell_type":"code","execution_count":null,"id":"2d325439-420d-4f9b-9e7e-5d78685e2a96","metadata":{},"outputs":[],"source":["BestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)"]},{"cell_type":"markdown","id":"c67b7fb7-d937-440f-8dd0-0d4f8ec88a8d","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #6): </h1>\n","Perform a grid search for the alpha parameter and the normalization parameter, then find the best values of the parameters:\n","</div>\n"]},{"cell_type":"code","execution_count":null,"id":"c2f61d48-3c25-42c4-987c-ae79e0878361","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"fab7daf5-c4b7-4425-8d2c-466836e9535b","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","parameters2 = [{'alpha': [0.001, 0.1, 1, 10, 100, 1000, 10000, 100000, 100000]}]\n","\n","Grid2 = GridSearchCV(Ridge(), parameters2, cv=4)\n","Grid2.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)\n","best_alpha = Grid2.best_params_['alpha']\n","best_ridge_model = Ridge(alpha=best_alpha)\n","best_ridge_model.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)\n","\n","\n","```\n","\n","</details>\n","\n"]},{"cell_type":"markdown","id":"8418243c-434f-4636-adbb-1785fbdccc83","metadata":{},"source":["### Thank you for completing this lab!\n","\n","\n","## Author\n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\" target=\"_blank\">Joseph Santarcangelo</a>\n","\n","\n","### Other Contributors\n","\n","<a href=\"https://www.linkedin.com/in/mahdi-noorian-58219234/\" target=\"_blank\">Mahdi Noorian PhD</a>\n","\n","Bahare Talayian\n","\n","Eric Xiao\n","\n","Steven Dong\n","\n","Parizad\n","\n","Hima Vasudevan\n","\n","<a href=\"https://www.linkedin.com/in/fiorellawever/\" target=\"_blank\">Fiorella Wenver</a>\n","\n","<a href=\" https://www.linkedin.com/in/yi-leng-yao-84451275/ \" target=\"_blank\" >Yi Yao</a>.\n","\n","\n","\n","## Change Log\n","\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-10-30  | 2.3  | Lakshmi  | Changed URL of csv              |\n","| 2020-10-05  | 2.2  | Lakshmi  | Removed unused library imports  |\n","| 2020-09-14  | 2.1  | Lakshmi  | Made changes in OverFitting section  |\n","| 2020-08-27  | 2.0  | Lavanya  |  Moved lab to course repo in GitLab  |\n","\n","\n","<hr>\n","\n","## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
